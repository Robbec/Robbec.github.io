
@article{han_body_2020,
	title = {The {Body} in {Cross}-{Reality}: {A} {Framework} for {Selective} {Augmented} {Reality} {Visualisation} of {Virtual} {Objects}},
	issn = {http://ceur-ws.org/Vol-2779/},
	abstract = {The body plays a communicative function in interaction. It expresses how we respond, experience and interact with the world through action, movement, and gestures. In this paper, we investigate the impact of the body in Cross-Reality Interaction between
users of dierent realities in the Reality-Virtuality continuum. We propose a Framework for Selective Augmented Reality Visualisation of Virtual Objects that enables an external Augmented Reality user to perceive an immersed Virtual Reality user against dierent
levels of information. The augmented reality user may observe the real body of the user in the context of visualised objects from the virtual environment, selected according to three criteria: Proximity Threshold, Field of View, and Importance Ranking. We aim to investigate how much and what type of virtual objects need to be visualised in order to convey clear information on the activity and physical engagement of the immersed Virtual Reality user. Two use cases are presented to which this framework can be applied: vocational training on food hygiene and a virtual exhibition for architecture.},
	urldate = {2021-01-28},
	author = {Han, Jihae and Cools, Robbe and Simeone, Adalberto L},
	year = {2020},
	keywords = {augmented reality, cross-reality interaction, virtual reality},
	file = {PDF:C\:\\Users\\Robbe\\Zotero\\storage\\FRCEY8XC\\full-text.pdf:application/pdf},
}

@inproceedings{cools_selectvisar_2021,
	title = {{SelectVisAR}: {Selective} {Visualisation} of {Virtual} {Environments} in {Augmented} {Reality}},
	isbn = {978-1-4503-8476-6},
	doi = {10.1145/3461778.3462096},
	abstract = {When establishing a visual connection between a virtual reality user and an augmented reality user, it is important to consider whether the augmented reality user faces a surplus of information. Augmented reality, compared to virtual reality, involves two - not one - planes of information: the physical and the virtual. We propose SelectVisAR, a selective visualisation system of virtual environments in augmented reality. Our system enables an augmented reality spectator to perceive a co-located virtual reality user in the context of four distinct visualisation conditions: Interactive, Proximity, Everything, and Dollhouse. We explore an additional two conditions, Context and Spotlight, in a follow-up study. Our design uses a human-centric approach to information filtering, selectively visualising only parts of the virtual environment related to the interactive possibilities of a virtual reality user. The research investigates how selective visualisations can be helpful or trivial for the augmented reality user when observing a virtual reality user.},
	urldate = {2021-10-11},
	booktitle = {{DIS} 2021 - {Proceedings} of the 2021 {ACM} {Designing} {Interactive} {Systems} {Conference}: {Nowhere} and {Everywhere}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Cools, Robbe and Han, Jihae and Simeone, Adalberto L.},
	month = jun,
	year = {2021},
	keywords = {virtual reality, augmented reality, cross-reality interaction},
	pages = {275--282},
	file = {PDF:C\:\\Users\\Robbe\\Zotero\\storage\\SATY78MQ\\full-text.pdf:application/pdf},
}

@inproceedings{cools_investigating_2019,
	address = {New York, NY, USA},
	series = {{SUI} '19},
	title = {Investigating the {Effect} of {Distractor} {Interactivity} for {Redirected} {Walking} in {Virtual} {Reality}},
	isbn = {978-1-4503-6975-6},
	url = {https://doi.org/10.1145/3357251.3357580},
	doi = {10.1145/3357251.3357580},
	abstract = {Due to the mismatch in size between a Virtual Environment and the physical space available, the use of alternative locomotion techniques becomes necessary. In small spaces, Redirected Walking methods provide limited benefits and approaches such as the use of distractors can provide an alternative. Distractors are virtual elements or characters that attempt to catch the attention of the user while the system subtly steers them away from physical boundaries. In this research we explicitly focused on understanding how different levels of interactivity affect user performance and behaviour. We developed three types of continuous redirecting distractors, with varying levels of interaction possibilities, called Looking, Touching, and Interacting. We compared them in a user study to a discrete reorientation technique, called Stop and Reset, in a task requiring users to traverse a 30 m path. While discrete reorientation is faster, continuous redirection through distractors was significantly less noticeable. Results suggest that more complex interaction is preferred and able to better captivate user attention for longer.},
	urldate = {2022-10-12},
	booktitle = {Symposium on {Spatial} {User} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Cools, Robbe and Simeone, Adalberto L.},
	month = oct,
	year = {2019},
	keywords = {Virtual Reality, Redirected Walking, distractors, interactivity},
	pages = {1--5},
	file = {Full Text PDF:C\:\\Users\\Robbe\\Zotero\\storage\\ET7WA9CD\\Cools and Simeone - 2019 - Investigating the Effect of Distractor Interactivi.pdf:application/pdf},
}

@inproceedings{simeone_immersive_2022,
	address = {New York, NY, USA},
	series = {{CHI} '22},
	title = {Immersive {Speculative} {Enactments}: {Bringing} {Future} {Scenarios} and {Technology} to {Life} {Using} {Virtual} {Reality}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Immersive {Speculative} {Enactments}},
	url = {https://doi.org/10.1145/3491102.3517492},
	doi = {10.1145/3491102.3517492},
	abstract = {In this paper we present Immersive Speculative Enactments (ISEs), a novel concept that extends conventional Speculative Enactments to Virtual Reality. Through ISEs, participants are immersed in a speculative world depicted by the designers and can engage with it in its truest envisioned form. We explore this concept via four scenarios with increasing technological uncertainty: a glimpse in the daily life of the parent of a newborn baby; a Mixed Reality experience supporting hybrid classrooms; two wearable devices that present a pet’s emotional state and needs; and an enactment on the effect of communication delay across interplanetary distances. We discuss the concept of ISEs and contrast them to other forms of speculation, provide guidelines on how to design them, as well as reflecting on the challenges, limitations, and potential associated with the role of ISEs in the HCI discourse.},
	urldate = {2022-10-12},
	booktitle = {Proceedings of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Simeone, Adalberto L. and Cools, Robbe and Depuydt, Stan and Gomes, João Maria and Goris, Piet and Grocott, Joseph and Esteves, Augusto and Gerling, Kathrin},
	month = apr,
	year = {2022},
	keywords = {Virtual Reality, Childcare, Cross-Reality, Design Fiction, Quantified Pets, Space Exploration., Speculative Enactments},
	pages = {1--20},
	file = {Full Text PDF:C\:\\Users\\Robbe\\Zotero\\storage\\USSFFVL5\\Simeone et al. - 2022 - Immersive Speculative Enactments Bringing Future .pdf:application/pdf},
}

@inproceedings{cools_blending_2022,
	title = {Blending {Spaces}: {Cross}-{Reality} {Interaction} {Techniques} for {Object} {Transitions} {Between} {Distinct} {Virtual} and {Augmented} {Realities}},
	shorttitle = {Blending {Spaces}},
	url = {https://ieeexplore.ieee.org/document/9994912/},
	doi = {10.1109/ISMAR55827.2022.00069},
	abstract = {Cross-Reality (CR) involves interaction between different modalities and levels of immersion such as Virtual and Augmented Reality, as we explore in this paper. Whereas previous work assumed similarity between their respective Virtual and Augmented Environment (VE and AE), we explore the case in which VE and AE are distinct. This gives rise to novel and critical problems, such as how to visualise and interact with the other environment. In this context we investigate the fundamental interaction of transitioning an object across environments, to which we contribute five interaction techniques. Two are inspired by literature: Virtual Magic Lens and Binary Transition; while the other three are entirely novel: Auto Blended Space, Manual Blended Space - Button Transition and Manual Blended Space - Touch Transition. In a study evaluating the first four techniques, we found that participants (N=20) performed a CR object manipulation and transition task significantly faster using our Auto Blended Space technique. We then modified Manual Blended Space - Button Transition into Manual Blended Space - Touch Transition in response to these results, and reassessed the four techniques in a more complex object manipulation task (N=16). We found that this type of task was better suited to manual transition methods rather than automatic methods. Taken together, our final contribution are five blended space design factors, and timely Cross-Reality transition design guidelines.},
	booktitle = {2022 {IEEE} {International} {Symposium} on {Mixed} and {Augmented} {Reality} ({ISMAR})},
	author = {Cools, Robbe and Esteves, Augusto and Simeone, Adalberto L.},
	month = oct,
	year = {2022},
	note = {ISSN: 1554-7868},
	keywords = {Augmented reality, Design methodology, Human-centered computing, Manuals, Mixed / augmented reality, Navigation, Task analysis, Virtual environments, Virtual reality, Visualization},
	pages = {528--537},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Robbe\\Zotero\\storage\\3TH7Y4PA\\9994912.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Robbe\\Zotero\\storage\\FY74ESP7\\Cools et al. - 2022 - Blending Spaces Cross-Reality Interaction Techniq.pdf:application/pdf},
}

@inproceedings{cools_towards_2022,
	title = {Towards a {Desktop}-{AR} {Prototyping} {Framework}: {Prototyping} {Cross}-{Reality} {Between} {Desktops} and {Augmented} {Reality}},
	shorttitle = {Towards a {Desktop}-{AR} {Prototyping} {Framework}},
	url = {https://ieeexplore.ieee.org/document/9974207},
	doi = {10.1109/ISMAR-Adjunct57072.2022.00040},
	abstract = {Augmented reality (AR) head-worn displays (HWDs) allow users to view and interact with virtual objects anchored in the 3D space around them. These devices extend users' digital interaction space compared to traditional desktop computing environments by both allowing users to interact with a larger virtual display and by affording new interactions (e.g., intuitive 3D manipulations) with virtual content. Yet, 2D desktop displays still have advantages over AR HWDs for common computing tasks and will continue to be used well into the future. Because of their not entirely overlapping set of affordances, AR HWDs and 2D desktops may be useful in a hybrid configuration; that is, users may benefit from being able to work on computing tasks in either environment (or simultaneously in both environments) while transitioning virtual content between them. In support of such computing environments, we propose a prototyping framework for bidirectional Cross-Reality interactions between a desktop and an AR HWD. We further implemented a proof-of-concept seamless Desktop-AR display space, and describe two concrete use cases for our framework. In future work we aim to further develop our proof-of-concept into the proposed framework.},
	urldate = {2023-10-13},
	booktitle = {2022 {IEEE} {International} {Symposium} on {Mixed} and {Augmented} {Reality} {Adjunct} ({ISMAR}-{Adjunct})},
	author = {Cools, Robbe and Gottsacker, Matt and Simeone, Adalberto and Bruder, Gerd and Welch, Greg and Feiner, Steven},
	month = oct,
	year = {2022},
	note = {ISSN: 2771-1110},
	pages = {175--182},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Robbe\\Zotero\\storage\\3AFYXH45\\9974207.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Robbe\\Zotero\\storage\\ZGK7ETD5\\Cools et al. - 2022 - Towards a Desktop-AR Prototyping Framework Protot.pdf:application/pdf},
}

@inproceedings{cools_mobile_2022,
	address = {New York, NY, USA},
	series = {{MUM} '21},
	title = {Mobile {Displays} for {Cross}-{Reality} {Interactions} between {Virtual} and {Physical} {Realities}},
	isbn = {978-1-4503-8643-2},
	url = {https://dl.acm.org/doi/10.1145/3490632.3497838},
	doi = {10.1145/3490632.3497838},
	abstract = {We present two use cases of mobile displays in cross-reality interactions between users immersed in Virtual Reality (VR) and users present in the Physical Reality (PR) by using the mobile display to show select artefacts of interest. The first use case is the “Substitutional Display” where a display serves as a passive haptic for an artefact. Both VR and PR users can then move the artefact by physically moving the display. The second use case is a “Virtual Artefact Handover” which allows the VR user to pass artefacts onto the PR user’s display. We envision this handover as a natural interaction where the VR user moves the artefact onto a virtual proxy of the display the PR user is holding, after which the artefact is displayed for the PR user to see.},
	urldate = {2023-10-20},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Mobile} and {Ubiquitous} {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Cools, Robbe and Simeone, Adalberto},
	month = feb,
	year = {2022},
	keywords = {cross-reality interaction, mobile displays, physical reality, virtual reality},
	pages = {217--219},
	file = {Full Text PDF:C\:\\Users\\Robbe\\Zotero\\storage\\S2RAMJI8\\Cools and Simeone - 2022 - Mobile Displays for Cross-Reality Interactions bet.pdf:application/pdf},
}

@inproceedings{zhang_arcoustic_2023,
	address = {New York, NY, USA},
	series = {{AutomotiveUI} '23},
	title = {{ARcoustic}: {A} {Mobile} {Augmented} {Reality} {System} for {Seeing} {Out}-of-{View} {Traffic}},
	isbn = {9798400701054},
	shorttitle = {{ARcoustic}},
	url = {https://dl.acm.org/doi/10.1145/3580585.3606461},
	doi = {10.1145/3580585.3606461},
	abstract = {Locating out-of-view vehicles can help pedestrians to avoid critical traffic encounters. Some previous approaches focused solely on visualising out-of-view objects, neglecting their localisation and limitations. Other methods rely on continuous camera-based localisation, raising privacy concerns. Hence, we propose the ARcoustic system, which utilises a microphone array for nearby moving vehicle localisation and visualises nearby out-of-view vehicles to support pedestrians. First, we present the implementation of our sonic-based localisation and discuss the current technical limitations. Next, we present a user study (n = 18) in which we compared two state-of-the-art visualisation techniques (Radar3D, CompassbAR) to a baseline without any visualisation. Results show that both techniques present too much information, resulting in below-average user experience and longer response times. Therefore, we introduce a novel visualisation technique that aligns with the technical localisation limitations and meets pedestrians’ preferences for effective visualisation, as demonstrated in the second user study (n = 16). Lastly, we conduct a small field study (n = 8) testing our ARcoustic system under realistic conditions. Our work shows that out-of-view object visualisations must align with the underlying localisation technology and fit the concrete application scenario.},
	urldate = {2023-10-20},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Xuesong and Wu, Xian and Cools, Robbe and Simeone, Adalberto L. and Gruenefeld, Uwe},
	month = sep,
	year = {2023},
	keywords = {AR, out-of-view, sonic-based localisation, visualisation, VR},
	pages = {178--190},
	file = {Full Text PDF:C\:\\Users\\Robbe\\Zotero\\storage\\HQ3L6MLC\\Zhang et al. - 2023 - ARcoustic A Mobile Augmented Reality System for S.pdf:application/pdf},
}

@inproceedings{de_bauw_adaptables_2022,
	address = {New York, NY, USA},
	series = {{SUI} '22},
	title = {{AdapTables}: {Using} {Conformal} {Mapping} for {Collaboration} on {Tables} in {Asymmetric} {Mixed} {Reality}},
	isbn = {978-1-4503-9948-7},
	shorttitle = {{AdapTables}},
	url = {https://dl.acm.org/doi/10.1145/3565970.3567691},
	doi = {10.1145/3565970.3567691},
	abstract = {Remote meeting applications are becoming more immersive by supporting virtual reality, however, support for augmented reality devices is still lingering. For augmented reality to be integrated, the asymmetry between the local spaces of the users needs to be solved, to which we contribute by focusing on the mismatch between tables. We present the AdapTables system, which maps a virtual reality user’s virtual meeting table onto an augmented reality user’s differently shaped physical table. By creating conformal maps between these tables, remote users can be transported to the environment of the other user. Additionally, shared virtual objects are also mapped to adapt to each user’s table. We tested the system with 32 participants in pairs of two.},
	urldate = {2023-10-20},
	booktitle = {Proceedings of the 2022 {ACM} {Symposium} on {Spatial} {User} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {De Bauw, Tim and Cools, Robbe and Simeone, Adalberto L.},
	month = dec,
	year = {2022},
	pages = {1--2},
	file = {Full Text PDF:C\:\\Users\\Robbe\\Zotero\\storage\\6S6RAH8F\\De Bauw et al. - 2022 - AdapTables Using Conformal Mapping for Collaborat.pdf:application/pdf},
}

@article{goos_computer_2021,
	series = {8th {CIRP} {Conference} of {Assembly} {Technology} and {Systems}},
	title = {Computer assisted ergonomic assembly cell design},
	volume = {97},
	issn = {2212-8271},
	url = {https://www.sciencedirect.com/science/article/pii/S221282712031427X},
	doi = {10.1016/j.procir.2020.05.208},
	abstract = {When designing or modifying an assembly cell, the question at hand is "What will it cost, and what will we gain"? This research features the evaluation of assembly cell variants, taking operator timing and ergonomics into account. For each assembly step requiring an operator e.g. using a screwdriver, the ergonomic score is evaluated by considering the (automatically generated) poses of the operator at the start and finish of each assembly action. Furthermore, throughput is estimated by applying Methods-Time Measurement (MTM) to an inverted kinematics model of a virtual operator. We present our findings on an industrial use case: the assembly of a 15 kg compressor. Starting from an existing cobot cell, alternative assembly cells, with an adjusted resource allocation and a modified cell layout, are evaluated. Besides the performance metrics, human design factors, that are hard to model mathematically, can be incorporated in the design by visualizing and experiencing the work cell in virtual reality. This allows to further fine-tune the cell design.},
	urldate = {2023-10-20},
	journal = {Procedia CIRP},
	author = {Goos, Jan and Lietaert, Pieter and Cools, Robbe},
	month = jan,
	year = {2021},
	keywords = {Human operator support \& ergonomics in assembly, Performance assessment of assembly systems},
	pages = {87--91},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Robbe\\Zotero\\storage\\TAT26PAP\\Goos et al. - 2021 - Computer assisted ergonomic assembly cell design.pdf:application/pdf},
}

@inproceedings{pazhayedath_exploring_2021,
	title = {Exploring {Bi}-{Directional} {Pinpointing} {Techniques} for {Cross}-{Reality} {Collaboration}},
	url = {https://ieeexplore.ieee.org/abstract/document/9419115},
	doi = {10.1109/VRW52623.2021.00055},
	abstract = {Virtual Reality (VR) technology enables users to immerse themselves in artificial worlds. However, it isolates users from the outside world and impedes them from collaborating with other users who might be outside of the VR experience and vice-versa. We implemented two systems where we explore how such an external user in the real world can interact across realities with a user immersed in virtual reality, either locally or remotely, in order to to share pinpoint locations. In the first we investigate three cross-reality techniques for the external user to draw the attention of their VR counterpart on specific objects present in the virtual environment (Voice, Highlight, and Arrow). Participants performed better overall and preferred the Arrow technique, followed by the Highlight technique. In the second system we expand on these two techniques to explore an even starker cross-reality interaction between users in VR and users interacting via a tablet computer to direct each other to pinpoint objects in the scene. We adapted the previous two techniques and implemented two others (Vision cone, Pointing) that support bi-directional communication between users. When it comes to bi-directional pinpointing, VR users still showed preference for the Arrow technique (now described as Pointing in Giant mode), while mobile users were split between the Vision cone and the Highlight techniques.},
	urldate = {2023-10-20},
	booktitle = {2021 {IEEE} {Conference} on {Virtual} {Reality} and {3D} {User} {Interfaces} {Abstracts} and {Workshops} ({VRW})},
	author = {Pazhayedath, Priyanka and Belchior, Pedro and Prates, Rafael and Silveira, Filipe and Lopes, Daniel Simões and Cools, Robbe and Esteves, Augusto and Simeone, Adalberto L.},
	month = mar,
	year = {2021},
	pages = {264--270},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Robbe\\Zotero\\storage\\PLWYDUUN\\Pazhayedath et al. - 2021 - Exploring Bi-Directional Pinpointing Techniques fo.pdf:application/pdf},
}
